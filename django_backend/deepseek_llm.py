"""
DeepSeek LLM åŒ…è£…ç±»
å…¼å®¹ llama-index çš„ LLM æ¥å£ï¼Œä½¿ç”¨ DeepSeek API
"""
import logging
import requests
from typing import Any, Dict, Optional, Sequence
from llama_index.core.llms import (
    LLM,
    ChatMessage,
    ChatResponse,
    ChatResponseGen,
    CompletionResponse,
    CompletionResponseGen,
    LLMMetadata,
)
from llama_index.core.llms.callbacks import llm_chat_callback, llm_completion_callback
from deepseek_config import get_api_key, DEEPSEEK_BASE_URL, DEFAULT_DEEPSEEK_MODEL, DEEPSEEK_API_PARAMS, DEEPSEEK_TIMEOUT

logger = logging.getLogger(__name__)


class DeepSeekLLM(LLM):
    """
    DeepSeek LLM åŒ…è£…ç±»ï¼Œå…¼å®¹ llama-index æ¥å£
    """
    
    def __init__(
        self,
        model: str = DEFAULT_DEEPSEEK_MODEL,
        api_key: Optional[str] = None,
        base_url: str = DEEPSEEK_BASE_URL,
        temperature: float = 0.1,
        max_tokens: int = 4096,
        timeout: int = DEEPSEEK_TIMEOUT,
        **kwargs: Any,
    ) -> None:
        """
        åˆå§‹åŒ– DeepSeek LLM
        
        Args:
            model: æ¨¡å‹åç§°
            api_key: API å¯†é’¥ï¼ˆå¦‚æœä¸æä¾›ï¼Œä»é…ç½®æ–‡ä»¶è¯»å–ï¼‰
            base_url: API åŸºç¡€åœ°å€
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§è¾“å‡ºé•¿åº¦
            timeout: è¶…æ—¶æ—¶é—´
        """
        super().__init__()
        self._model = model
        self._api_key = api_key or get_api_key()
        self._base_url = base_url
        self._temperature = temperature
        self._max_tokens = max_tokens
        self._timeout = timeout
        
        if not self._api_key:
            raise ValueError(
                "æœªæ‰¾åˆ° DeepSeek API Keyï¼è¯·é€šè¿‡ä»¥ä¸‹æ–¹å¼ä¹‹ä¸€è®¾ç½®ï¼š\n"
                "1. è®¾ç½®ç¯å¢ƒå˜é‡: export DEEPSEEK_API_KEY='your-api-key'\n"
                "2. åˆ›å»º .env æ–‡ä»¶ï¼Œæ·»åŠ : DEEPSEEK_API_KEY=your-api-key\n"
                "3. åœ¨åˆå§‹åŒ–æ—¶ä¼ å…¥: DeepSeekLLM(api_key='your-api-key')"
            )
        
        logger.info(f"âœ… DeepSeek LLM åˆå§‹åŒ–æˆåŠŸ - æ¨¡å‹: {self._model}")
    
    @property
    def metadata(self) -> LLMMetadata:
        """è¿”å› LLM å…ƒæ•°æ®"""
        return LLMMetadata(
            context_window=32768,  # DeepSeek ä¸Šä¸‹æ–‡çª—å£
            num_output=self._max_tokens,
            model_name=self._model,
        )
    
    @llm_chat_callback()
    def chat(self, messages: Sequence[ChatMessage], **kwargs: Any) -> ChatResponse:
        """
        åŒæ­¥èŠå¤©æ¥å£
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            ChatResponse å¯¹è±¡
        """
        # è½¬æ¢æ¶ˆæ¯æ ¼å¼
        api_messages = []
        for msg in messages:
            api_messages.append({
                "role": msg.role,
                "content": msg.content,
            })
        
        # è°ƒç”¨ DeepSeek API
        headers = {
            "Authorization": f"Bearer {self._api_key}",
            "Content-Type": "application/json",
        }
        
        payload = {
            "model": self._model,
            "messages": api_messages,
            "temperature": kwargs.get("temperature", self._temperature),
            "max_tokens": kwargs.get("max_tokens", self._max_tokens),
            "stream": False,
        }
        
        try:
            logger.info(f"ğŸš€ è°ƒç”¨ DeepSeek API - æ¨¡å‹: {self._model}")
            response = requests.post(
                f"{self._base_url}/v1/chat/completions",
                headers=headers,
                json=payload,
                timeout=self._timeout,
            )
            response.raise_for_status()
            
            result = response.json()
            content = result["choices"][0]["message"]["content"]
            
            logger.info(f"âœ… DeepSeek API å“åº”æˆåŠŸ - é•¿åº¦: {len(content)} å­—ç¬¦")
            
            return ChatResponse(
                message=ChatMessage(role="assistant", content=content),
                raw=result,
            )
        
        except requests.exceptions.Timeout:
            logger.error(f"âŒ DeepSeek API è¶…æ—¶ - è¶…æ—¶æ—¶é—´: {self._timeout}ç§’")
            raise Exception(f"DeepSeek API è°ƒç”¨è¶…æ—¶ï¼ˆ{self._timeout}ç§’ï¼‰")
        
        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ DeepSeek API è¯·æ±‚å¤±è´¥: {e}")
            if hasattr(e.response, 'text'):
                logger.error(f"é”™è¯¯è¯¦æƒ…: {e.response.text}")
            raise Exception(f"DeepSeek API è°ƒç”¨å¤±è´¥: {e}")
    
    @llm_completion_callback()
    def complete(self, prompt: str, **kwargs: Any) -> CompletionResponse:
        """
        åŒæ­¥è¡¥å…¨æ¥å£ï¼ˆé€šè¿‡ chat å®ç°ï¼‰
        
        Args:
            prompt: æç¤ºè¯
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            CompletionResponse å¯¹è±¡
        """
        messages = [ChatMessage(role="user", content=prompt)]
        chat_response = self.chat(messages, **kwargs)
        
        return CompletionResponse(
            text=chat_response.message.content,
            raw=chat_response.raw,
        )
    
    def stream_chat(
        self, messages: Sequence[ChatMessage], **kwargs: Any
    ) -> ChatResponseGen:
        """æµå¼èŠå¤©"""
        # è½¬æ¢æ¶ˆæ¯æ ¼å¼
        api_messages = []
        for msg in messages:
            api_messages.append({
                "role": msg.role,
                "content": msg.content,
            })
        
        # è°ƒç”¨ DeepSeek API (æµå¼)
        headers = {
            "Authorization": f"Bearer {self._api_key}",
            "Content-Type": "application/json",
        }
        
        payload = {
            "model": self._model,
            "messages": api_messages,
            "temperature": kwargs.get("temperature", self._temperature),
            "max_tokens": kwargs.get("max_tokens", self._max_tokens),
            "stream": True,  # å¼€å¯æµå¼
        }
        
        try:
            logger.info(f"ğŸš€ è°ƒç”¨ DeepSeek API (æµå¼) - æ¨¡å‹: {self._model}")
            response = requests.post(
                f"{self._base_url}/v1/chat/completions",
                headers=headers,
                json=payload,
                timeout=self._timeout,
                stream=True,  # æµå¼å“åº”
            )
            response.raise_for_status()
            
            # ç”Ÿæˆå™¨ï¼šé€å—è¿”å›
            def gen():
                full_text = ""
                for line in response.iter_lines():
                    if line:
                        line_text = line.decode('utf-8')
                        if line_text.startswith('data: '):
                            data_text = line_text[6:]  # å»æ‰ 'data: '
                            if data_text == '[DONE]':
                                break
                            try:
                                import json
                                data = json.loads(data_text)
                                delta = data['choices'][0].get('delta', {})
                                content = delta.get('content', '')
                                if content:
                                    full_text += content
                                    yield ChatResponse(
                                        message=ChatMessage(role="assistant", content=full_text),
                                        delta=content,
                                        raw=data,
                                    )
                            except:
                                continue
            
            return gen()
        
        except requests.exceptions.Timeout:
            logger.error(f"âŒ DeepSeek API è¶…æ—¶ - è¶…æ—¶æ—¶é—´: {self._timeout}ç§’")
            raise Exception(f"DeepSeek API è°ƒç”¨è¶…æ—¶ï¼ˆ{self._timeout}ç§’ï¼‰")
        
        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ DeepSeek API è¯·æ±‚å¤±è´¥: {e}")
            if hasattr(e.response, 'text'):
                logger.error(f"é”™è¯¯è¯¦æƒ…: {e.response.text}")
            raise Exception(f"DeepSeek API è°ƒç”¨å¤±è´¥: {e}")
    
    def stream_complete(self, prompt: str, **kwargs: Any) -> CompletionResponseGen:
        """æµå¼è¡¥å…¨ï¼ˆæš‚ä¸æ”¯æŒï¼‰"""
        raise NotImplementedError("DeepSeek æµå¼è¡¥å…¨æš‚æœªå®ç°")
    
    async def achat(
        self, messages: Sequence[ChatMessage], **kwargs: Any
    ) -> ChatResponse:
        """å¼‚æ­¥èŠå¤©ï¼ˆæš‚ä¸æ”¯æŒï¼Œè°ƒç”¨åŒæ­¥ç‰ˆæœ¬ï¼‰"""
        return self.chat(messages, **kwargs)
    
    async def acomplete(self, prompt: str, **kwargs: Any) -> CompletionResponse:
        """å¼‚æ­¥è¡¥å…¨ï¼ˆæš‚ä¸æ”¯æŒï¼Œè°ƒç”¨åŒæ­¥ç‰ˆæœ¬ï¼‰"""
        return self.complete(prompt, **kwargs)
    
    async def astream_chat(
        self, messages: Sequence[ChatMessage], **kwargs: Any
    ) -> ChatResponseGen:
        """å¼‚æ­¥æµå¼èŠå¤©ï¼ˆæš‚ä¸æ”¯æŒï¼‰"""
        raise NotImplementedError("DeepSeek å¼‚æ­¥æµå¼èŠå¤©æš‚æœªå®ç°")
    
    async def astream_complete(
        self, prompt: str, **kwargs: Any
    ) -> CompletionResponseGen:
        """å¼‚æ­¥æµå¼è¡¥å…¨ï¼ˆæš‚ä¸æ”¯æŒï¼‰"""
        raise NotImplementedError("DeepSeek å¼‚æ­¥æµå¼è¡¥å…¨æš‚æœªå®ç°")


def create_deepseek_embedding(model_name: str = "nomic-embed-text", **kwargs):
    """
    åˆ›å»º Embedding æ¨¡å‹
    æ³¨æ„ï¼šDeepSeek ç›®å‰ä¸æä¾› Embedding APIï¼Œè¿™é‡Œä½¿ç”¨æœ¬åœ° Ollama
    
    è¿”å› OllamaEmbedding å®ä¾‹ï¼ˆå…¼å®¹ llama-indexï¼‰
    """
    from llama_index.embeddings.ollama import OllamaEmbedding
    
    logger.warning(
        "âš ï¸  DeepSeek æš‚ä¸æä¾› Embedding APIï¼ŒEmbedding åŠŸèƒ½ä»ä½¿ç”¨ Ollama\n"
        f"   ä½¿ç”¨æ¨¡å‹: {model_name}\n"
        "   è¯·ç¡®ä¿ Ollama æœåŠ¡æ­£åœ¨è¿è¡Œ: ollama serve"
    )
    
    return OllamaEmbedding(
        model_name=model_name,
        request_timeout=300.0
    )

