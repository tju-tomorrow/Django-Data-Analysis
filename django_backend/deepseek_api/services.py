import time
import threading
from typing import Dict, Any, Optional
from django.core.cache import cache
import hashlib
from .models import APIKey, RateLimit, ConversationSession
from django.conf import settings

# å…¨å±€é…ç½®
# API_KEY_LENGTH = 32
# TOKEN_EXPIRY_SECONDS = 3600
# RATE_LIMIT_MAX = 5  # æ¯åˆ†é’Ÿæœ€å¤§è¯·æ±‚æ•°
# RATE_LIMIT_INTERVAL = 60

# çº¿ç¨‹é”ç”¨äºé€Ÿç‡é™åˆ¶
rate_lock = threading.Lock()

# å…¨å±€å•ä¾‹ï¼šTopKLogSystem å®ä¾‹ï¼ˆæ‡’åŠ è½½ï¼‰
_log_system_instance = None
_log_system_lock = threading.Lock()

def get_log_system():
    """
    è·å– TopKLogSystem å•ä¾‹å®ä¾‹ï¼ˆæ‡’åŠ è½½ + çº¿ç¨‹å®‰å…¨ï¼‰
    åªåœ¨ç¬¬ä¸€æ¬¡è°ƒç”¨æ—¶åˆå§‹åŒ–ï¼Œåç»­ç›´æ¥è¿”å›å·²æœ‰å®ä¾‹
    """
    global _log_system_instance
    
    if _log_system_instance is None:
        with _log_system_lock:
            # åŒé‡æ£€æŸ¥é”å®šæ¨¡å¼ï¼ˆé¿å…å¤šçº¿ç¨‹é‡å¤åˆå§‹åŒ–ï¼‰
            if _log_system_instance is None:
                from topklogsystem import TopKLogSystem
                from model_config import CURRENT_CONFIG
                import logging
                logger = logging.getLogger(__name__)
                
                logger.info("åˆå§‹åŒ– TopKLogSystem å•ä¾‹å®ä¾‹...")
                logger.info(f"ä½¿ç”¨æ¨¡å‹: LLM={CURRENT_CONFIG['llm']}, Embedding={CURRENT_CONFIG['embedding_model']}")
                
                # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ API
                use_api = CURRENT_CONFIG.get('use_api', False)
                if use_api:
                    logger.info("ğŸŒ é…ç½®ä¸ºä½¿ç”¨ DeepSeek API")
                else:
                    logger.info("ğŸ–¥ï¸  é…ç½®ä¸ºä½¿ç”¨æœ¬åœ° Ollama")
                
                _log_system_instance = TopKLogSystem(
                    log_path="./data/log",
                    llm=CURRENT_CONFIG['llm'],
                    embedding_model=CURRENT_CONFIG['embedding_model'],
                    use_api=use_api  # ä¼ é€’ API ä½¿ç”¨æ ‡å¿—
                )
                logger.info("TopKLogSystem åˆå§‹åŒ–å®Œæˆï¼")
    
    return _log_system_instance

def deepseek_r1_api_call(prompt: str, query_type: str = "analysis") -> str:
    """
    è°ƒç”¨ DeepSeek API
    
    Args:
        prompt: ç”¨æˆ·è¾“å…¥çš„é—®é¢˜
        query_type: æŸ¥è¯¢ç±»å‹ï¼ˆanalysis: æ—¥å¿—åˆ†æ, general_chat: æ—¥å¸¸èŠå¤©ï¼‰
    
    Returns:
        LLM çš„å“åº”æ–‡æœ¬
    """
    print(f"\nğŸ¤– [å¤§æ¨¡å‹è°ƒç”¨] å¼€å§‹è°ƒç”¨ DeepSeek API")
    print(f"ğŸ¤– [è°ƒç”¨å‚æ•°] query_type: '{query_type}'")
    print(f"ğŸ¤– [Prompté•¿åº¦] {len(prompt)} å­—ç¬¦")
    
    # å…ˆè¿›è¡Œæ„å›¾åˆ†ç±»ï¼Œæ£€æŸ¥æ˜¯å¦ä¸ºå·¥å…·ç±»æ„å›¾
    from .intent_classifier import get_intent_classifier, TOOL_INTENTS
    classifier = get_intent_classifier()
    intent_result = classifier.classify_intent(prompt)
    
    print(f"ğŸ” [æ„å›¾åˆ†ç±»] æ„å›¾: {intent_result.intent.value}, ç½®ä¿¡åº¦: {intent_result.confidence:.3f}")
    
    # å¦‚æœæ˜¯å·¥å…·ç±»æ„å›¾ï¼Œæ‰§è¡Œå·¥å…·å¹¶å°†ç»“æœä¼ é€’ç»™LLM
    tool_result = None
    if intent_result.intent in TOOL_INTENTS:
        print(f"ğŸ”§ [å·¥å…·è°ƒç”¨] æ£€æµ‹åˆ°å·¥å…·ç±»æ„å›¾: {intent_result.intent.value}")
        tool_func = classifier.tools.get(intent_result.intent)
        if tool_func:
            tool_result = tool_func(prompt)
            print(f"âœ… [å·¥å…·æ‰§è¡Œ] å·¥å…·æ‰§è¡Œå®Œæˆï¼Œç»“æœé•¿åº¦: {len(tool_result)} å­—ç¬¦")
            print(f"ğŸ”§ [å·¥å…·ç»“æœ] å°†å·¥å…·ç»“æœä¼ é€’ç»™LLMè¿›è¡Œåˆ†æå’Œå›ç­”")
        else:
            print(f"âš ï¸ [å·¥å…·è°ƒç”¨] æœªæ‰¾åˆ°å¯¹åº”çš„å·¥å…·å‡½æ•°: {intent_result.intent.value}")
    
    from model_config import CURRENT_CONFIG
    use_api = CURRENT_CONFIG.get('use_api', False)
    
    if use_api:
        # ä½¿ç”¨ DeepSeek API
        from deepseek_llm import DeepSeekLLM
        from llama_index.core.llms import ChatMessage
        
        # å¦‚æœæ‰§è¡Œäº†å·¥å…·ï¼Œå°†å·¥å…·ç»“æœä½œä¸ºä¸Šä¸‹æ–‡ä¼ é€’ç»™LLM
        if tool_result:
            # æ„å»ºåŒ…å«å·¥å…·ç»“æœçš„prompt
            enhanced_prompt = f"""ç”¨æˆ·é—®é¢˜ï¼š{prompt}

å·¥å…·æ‰§è¡Œç»“æœï¼š
{tool_result}

è¯·åŸºäºä»¥ä¸Šå·¥å…·æ‰§è¡Œç»“æœï¼Œå¯¹ç”¨æˆ·çš„é—®é¢˜è¿›è¡Œè¯¦ç»†åˆ†æå’Œå›ç­”ã€‚è¦æ±‚ï¼š
1. å¯¹å·¥å…·ç»“æœè¿›è¡Œæ€»ç»“å’Œåˆ†æ
2. æŒ‡å‡ºå…³é”®é—®é¢˜å’Œå¼‚å¸¸
3. æä¾›å…·ä½“çš„å»ºè®®å’Œè§£å†³æ–¹æ¡ˆ
4. ç”¨æ¸…æ™°ã€ä¸“ä¸šçš„æ–¹å¼ç»„ç»‡å›ç­”"""
            
            print(f"ğŸ¤– [å·¥å…·å¢å¼ºPrompt] æ„å»ºå®Œæˆï¼Œé•¿åº¦: {len(enhanced_prompt)} å­—ç¬¦")
            llm = DeepSeekLLM(model=CURRENT_CONFIG['llm'], timeout=60)
            messages = [ChatMessage(role="user", content=enhanced_prompt)]
            
            print(f"ğŸ¤– [APIè¯·æ±‚] å‘é€å·¥å…·å¢å¼ºçš„è¯·æ±‚åˆ°å¤§æ¨¡å‹...")
            response = llm.chat(messages)
            
            result_text = response.message.content
            print(f"ğŸ¤– [APIå“åº”] æ”¶åˆ°å›å¤ï¼Œé•¿åº¦: {len(result_text)} å­—ç¬¦")
            print(f"ğŸ¤– [å›å¤å†…å®¹] {result_text[:100]}{'...' if len(result_text) > 100 else ''}")
            
            return result_text
        
        # æ ¹æ® query_type å†³å®šæ˜¯å¦ä½¿ç”¨ RAG
        if query_type == "analysis":
            # æ—¥å¿—åˆ†ææ¨¡å¼ï¼šä½¿ç”¨ RAG
            print(f"ğŸ¤– [RAGæ¨¡å¼] æ—¥å¿—åˆ†æï¼Œä½¿ç”¨ RAG æ£€ç´¢")
            system = get_log_system()
            
            print(f"ğŸ¤– [APIè¯·æ±‚] å‘é€è¯·æ±‚åˆ°å¤§æ¨¡å‹...")
            result = system.query(prompt, query_type=query_type)
            time.sleep(0.5)
            
            response = result["response"]
            print(f"ğŸ¤– [APIå“åº”] æ”¶åˆ°å›å¤ï¼Œé•¿åº¦: {len(response)} å­—ç¬¦")
            print(f"ğŸ¤– [å›å¤å†…å®¹] {response[:100]}{'...' if len(response) > 100 else ''}")
            
            return response
        else:
            # æ—¥å¸¸èŠå¤©æ¨¡å¼ï¼šç›´æ¥è°ƒç”¨ APIï¼Œä¸ä½¿ç”¨ RAG
            print(f"ğŸ¤– [çº¯å¯¹è¯æ¨¡å¼] æ—¥å¸¸èŠå¤©ï¼Œç›´æ¥è°ƒç”¨ DeepSeek APIï¼Œä¸ä½¿ç”¨ RAG æ£€ç´¢")
            
            llm = DeepSeekLLM(model=CURRENT_CONFIG['llm'], timeout=60)
            messages = [ChatMessage(role="user", content=prompt)]
            
            print(f"ğŸ¤– [APIè¯·æ±‚] å‘é€è¯·æ±‚åˆ°å¤§æ¨¡å‹...")
            response = llm.chat(messages)
            
            result_text = response.message.content
            print(f"ğŸ¤– [APIå“åº”] æ”¶åˆ°å›å¤ï¼Œé•¿åº¦: {len(result_text)} å­—ç¬¦")
            print(f"ğŸ¤– [å›å¤å†…å®¹] {result_text[:100]}{'...' if len(result_text) > 100 else ''}")
            
            return result_text
    else:
        # ä½¿ç”¨æœ¬åœ° Ollama + RAG ç³»ç»Ÿ
        print(f"ğŸ¤– [RAGæ¨¡å¼] ä½¿ç”¨æœ¬åœ° Ollama + RAG æ£€ç´¢")
        system = get_log_system()
        
        print(f"ğŸ¤– [APIè¯·æ±‚] å‘é€è¯·æ±‚åˆ°å¤§æ¨¡å‹...")
        result = system.query(prompt, query_type=query_type)
        time.sleep(0.5)
        
        response = result["response"]
        print(f"ğŸ¤– [APIå“åº”] æ”¶åˆ°å›å¤ï¼Œé•¿åº¦: {len(response)} å­—ç¬¦")
        print(f"ğŸ¤– [å›å¤å†…å®¹] {response[:100]}{'...' if len(response) > 100 else ''}")
        
        return response

def deepseek_r1_api_call_stream(prompt: str, query_type: str = "analysis", history_context: str = ""):
    """
    æµå¼è°ƒç”¨ DeepSeek APIï¼ˆæ”¯æŒ RAG å’Œå†å²ä¸Šä¸‹æ–‡ï¼‰
    
    Args:
        prompt: ç”¨æˆ·è¾“å…¥çš„é—®é¢˜
        query_type: æŸ¥è¯¢ç±»å‹ï¼ˆanalysis: æ—¥å¿—åˆ†æ, general_chat: æ—¥å¸¸èŠå¤©ï¼‰
        history_context: å†å²å¯¹è¯ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
    
    Yields:
        æµå¼å“åº”çš„ç”Ÿæˆå™¨
    """
    print(f"\nğŸ¤– [æµå¼è°ƒç”¨] å¼€å§‹æµå¼è°ƒç”¨ DeepSeek API")
    print(f"ğŸ¤– [è°ƒç”¨å‚æ•°] query_type: '{query_type}'")
    print(f"ğŸ¤– [Prompté•¿åº¦] {len(prompt)} å­—ç¬¦")
    print(f"ğŸ¤– [å†å²ä¸Šä¸‹æ–‡é•¿åº¦] {len(history_context)} å­—ç¬¦")
    
    # å…ˆè¿›è¡Œæ„å›¾åˆ†ç±»ï¼Œæ£€æŸ¥æ˜¯å¦ä¸ºå·¥å…·ç±»æ„å›¾
    from .intent_classifier import get_intent_classifier, TOOL_INTENTS
    classifier = get_intent_classifier()
    intent_result = classifier.classify_intent(prompt)
    
    print(f"ğŸ” [æ„å›¾åˆ†ç±»] æ„å›¾: {intent_result.intent.value}, ç½®ä¿¡åº¦: {intent_result.confidence:.3f}")
    
    # å¦‚æœæ˜¯å·¥å…·ç±»æ„å›¾ï¼Œæ‰§è¡Œå·¥å…·å¹¶å°†ç»“æœä¼ é€’ç»™LLM
    tool_result = None
    if intent_result.intent in TOOL_INTENTS:
        print(f"ğŸ”§ [å·¥å…·è°ƒç”¨] æ£€æµ‹åˆ°å·¥å…·ç±»æ„å›¾: {intent_result.intent.value}")
        tool_func = classifier.tools.get(intent_result.intent)
        if tool_func:
            tool_result = tool_func(prompt)
            print(f"âœ… [å·¥å…·æ‰§è¡Œ] å·¥å…·æ‰§è¡Œå®Œæˆï¼Œç»“æœé•¿åº¦: {len(tool_result)} å­—ç¬¦")
            print(f"ğŸ”§ [å·¥å…·ç»“æœ] å°†å·¥å…·ç»“æœä¼ é€’ç»™LLMè¿›è¡Œåˆ†æå’Œæµå¼å›ç­”")
        else:
            print(f"âš ï¸ [å·¥å…·è°ƒç”¨] æœªæ‰¾åˆ°å¯¹åº”çš„å·¥å…·å‡½æ•°: {intent_result.intent.value}")
    
    from model_config import CURRENT_CONFIG
    use_api = CURRENT_CONFIG.get('use_api', False)
    
    if not use_api:
        raise Exception("æµå¼è¾“å‡ºä»…æ”¯æŒ API æ¨¡å¼")
    
    from deepseek_llm import DeepSeekLLM
    from llama_index.core.llms import ChatMessage
    
    # è§£æå†å²ä¸Šä¸‹æ–‡ï¼Œæ„å»ºæ¶ˆæ¯åˆ—è¡¨
    messages = []
    if history_context:
        print(f"ğŸ¤– [å†å²è§£æ] è§£æå†å²å¯¹è¯...")
        from .conversation_manager import ConversationManager
        conversation_manager = ConversationManager(max_context_length=4000, max_turns=10)
        
        # è§£æå¹¶å‹ç¼©å†å²
        historical_turns = conversation_manager.parse_conversation_history(history_context)
        compressed_turns = conversation_manager.compress_context(historical_turns)
        
        print(f"ğŸ¤– [å†å²å‹ç¼©] åŸå§‹è½®æ¬¡: {len(historical_turns)}, å‹ç¼©å: {len(compressed_turns)}")
        
        # å°†å†å²è½¬æ¢ä¸ºæ¶ˆæ¯åˆ—è¡¨
        for turn in compressed_turns:
            messages.append(ChatMessage(role="user", content=turn.user_input))
            messages.append(ChatMessage(role="assistant", content=turn.assistant_reply))
    
    # å¦‚æœæ‰§è¡Œäº†å·¥å…·ï¼Œå°†å·¥å…·ç»“æœä½œä¸ºä¸Šä¸‹æ–‡ä¼ é€’ç»™LLMè¿›è¡Œæµå¼å›ç­”
    if tool_result:
        # æ„å»ºåŒ…å«å·¥å…·ç»“æœçš„prompt
        enhanced_prompt = f"""ç”¨æˆ·é—®é¢˜ï¼š{prompt}

å·¥å…·æ‰§è¡Œç»“æœï¼š
{tool_result}

è¯·åŸºäºä»¥ä¸Šå·¥å…·æ‰§è¡Œç»“æœï¼Œå¯¹ç”¨æˆ·çš„é—®é¢˜è¿›è¡Œè¯¦ç»†åˆ†æå’Œå›ç­”ã€‚è¦æ±‚ï¼š
1. å¯¹å·¥å…·ç»“æœè¿›è¡Œæ€»ç»“å’Œåˆ†æ
2. æŒ‡å‡ºå…³é”®é—®é¢˜å’Œå¼‚å¸¸
3. æä¾›å…·ä½“çš„å»ºè®®å’Œè§£å†³æ–¹æ¡ˆ
4. ç”¨æ¸…æ™°ã€ä¸“ä¸šçš„æ–¹å¼ç»„ç»‡å›ç­”"""
        
        print(f"ğŸ¤– [å·¥å…·å¢å¼ºPrompt] æ„å»ºå®Œæˆï¼Œé•¿åº¦: {len(enhanced_prompt)} å­—ç¬¦")
        messages.append(ChatMessage(role="user", content=enhanced_prompt))
        
        # æµå¼è°ƒç”¨ LLM
        llm = DeepSeekLLM(model=CURRENT_CONFIG['llm'], timeout=120)
        print(f"ğŸ¤– [æµå¼ç”Ÿæˆ] å¼€å§‹åŸºäºå·¥å…·ç»“æœæµå¼ç”Ÿæˆå›å¤...")
        return llm.stream_chat(messages)
    
    # æ ¹æ® query_type å†³å®šæ˜¯å¦ä½¿ç”¨ RAG
    if query_type == "analysis":
        # æ—¥å¿—åˆ†ææ¨¡å¼ï¼šä½¿ç”¨ RAG æ£€ç´¢ï¼Œç„¶åæµå¼ç”Ÿæˆ
        print(f"ğŸ¤– [RAGæ¨¡å¼] æ—¥å¿—åˆ†æï¼Œå…ˆè¿›è¡Œ RAG æ£€ç´¢...")
        system = get_log_system()
        
        # 1. RAG æ£€ç´¢ç›¸å…³æ—¥å¿—
        log_results = system.retrieve_logs(prompt)
        print(f"ğŸ¤– [RAGæ£€ç´¢] æ£€ç´¢åˆ° {len(log_results)} æ¡ç›¸å…³æ—¥å¿—")
        
        # 2. æ„å»ºåŒ…å«æ£€ç´¢ç»“æœçš„ prompt
        rag_prompt = system._build_prompt_string(prompt, log_results, query_type)
        print(f"ğŸ¤– [RAG Prompt] æ„å»ºå®Œæˆï¼Œé•¿åº¦: {len(rag_prompt)} å­—ç¬¦")
        
        # 3. æ·»åŠ å½“å‰ç”¨æˆ·è¾“å…¥ï¼ˆä½¿ç”¨RAGå¢å¼ºçš„promptï¼‰
        messages.append(ChatMessage(role="user", content=rag_prompt))
        
        print(f"ğŸ¤– [æ¶ˆæ¯åˆ—è¡¨] æ€»æ¶ˆæ¯æ•°: {len(messages)} (åŒ…å«å†å²)")
    else:
        # æ—¥å¸¸èŠå¤©æ¨¡å¼ï¼šç›´æ¥æ·»åŠ ç”¨æˆ·è¾“å…¥
        print(f"ğŸ¤– [çº¯å¯¹è¯æ¨¡å¼] æ—¥å¸¸èŠå¤©ï¼Œç›´æ¥æµå¼è°ƒç”¨...")
        messages.append(ChatMessage(role="user", content=prompt))
        
        print(f"ğŸ¤– [æ¶ˆæ¯åˆ—è¡¨] æ€»æ¶ˆæ¯æ•°: {len(messages)} (åŒ…å«å†å²)")
    
    # æµå¼è°ƒç”¨ LLM
    llm = DeepSeekLLM(model=CURRENT_CONFIG['llm'], timeout=120)
    print(f"ğŸ¤– [æµå¼ç”Ÿæˆ] å¼€å§‹æµå¼ç”Ÿæˆå›å¤...")
    return llm.stream_chat(messages)

def create_api_key(user: str) -> str:
    """åˆ›å»º API Key å¹¶ä¿å­˜åˆ°æ•°æ®åº“"""
    key = APIKey.generate_key()
    expiry = time.time() + settings.TOKEN_EXPIRY_SECONDS
    
    api_key = APIKey.objects.create(
        key=key,
        user=user,
        expiry_time=expiry
    )
    
    # åˆ›å»ºå¯¹åº”çš„é€Ÿç‡é™åˆ¶è®°å½•
    RateLimit.objects.create(
        api_key=api_key,
        reset_time=time.time() + settings.RATE_LIMIT_INTERVAL
    )
    
    return key

def validate_api_key(key_str: str) -> bool:
    """éªŒè¯ API Key æ˜¯å¦å­˜åœ¨ä¸”æœªè¿‡æœŸ"""
    try:
        api_key = APIKey.objects.get(key=key_str)
        if api_key.is_valid():
            return True
        else:
            api_key.delete()  # åˆ é™¤è¿‡æœŸkey
            return False
    except APIKey.DoesNotExist:
        return False

def check_rate_limit(key_str: str) -> bool:
    """æ£€æŸ¥ API Key çš„è¯·æ±‚é¢‘ç‡æ˜¯å¦è¶…è¿‡é™åˆ¶"""
    with rate_lock:
        try:
            # api_key = APIKey.objects.get(key=key_str)
            # rate_limit = RateLimit.objects.get(api_key=api_key)
            rate_limit = RateLimit.objects.select_related('api_key').get(api_key__key=key_str)
            
            current_time = time.time()
            if current_time > rate_limit.reset_time:
                rate_limit.count = 1
                rate_limit.reset_time = current_time + settings.RATE_LIMIT_INTERVAL
                rate_limit.save()
                return True
            elif rate_limit.count < settings.RATE_LIMIT_MAX:
                rate_limit.count += 1
                rate_limit.save()
                return True
            else:
                return False
        except RateLimit.DoesNotExist:
            # å¦‚æœé€Ÿç‡é™åˆ¶è®°å½•ä¸å­˜åœ¨ï¼Œåˆ›å»ºä¸€ä¸ªæ–°çš„
            try:
                current_time = time.time()
                api_key = APIKey.objects.get(key=key_str)
                RateLimit.objects.create(
                    api_key=api_key,
                    count=1,
                    reset_time=current_time + settings.RATE_LIMIT_INTERVAL
                )
                return True
            except APIKey.DoesNotExist:
                return False

# def get_or_create_session(session_id: str, user: APIKey) -> ConversationSession:
    # """è·å–æˆ–åˆ›å»ºä¼šè¯ï¼Œå…³è”å½“å‰ç”¨æˆ·ï¼ˆé€šè¿‡API Keyï¼‰"""
    # session, created = ConversationSession.objects.get_or_create(
        # session_id=session_id,
        # user=user,  # ç»‘å®šç”¨æˆ·
        # defaults={'context': ''}
    # )
    # return session

def get_or_create_session(session_id: str, user: APIKey) -> ConversationSession:
    """
    è·å–æˆ–åˆ›å»ºç”¨æˆ·çš„ä¸“å±ä¼šè¯ï¼š
    - è‹¥ç”¨æˆ·+session_idå·²å­˜åœ¨ â†’ åŠ è½½æ—§ä¼šè¯ï¼ˆä¿ç•™å†å²ï¼‰
    - è‹¥ä¸å­˜åœ¨ â†’ åˆ›å»ºæ–°ä¼šè¯ï¼ˆç©ºå†å²ï¼‰
    """
    print(f"ğŸ” [æ•°æ®åº“æŸ¥è¯¢] æŸ¥æ‰¾ä¼šè¯: session_id='{session_id}', user='{user.user}'")
    
    session, created = ConversationSession.objects.get_or_create(
        session_id=session_id,  # åŒ¹é…ä¼šè¯ID
        user=user,              # åŒ¹é…å½“å‰ç”¨æˆ·ï¼ˆå…³é”®ï¼é¿å…è·¨ç”¨æˆ·ä¼šè¯å†²çªï¼‰
        defaults={'context': ''}
    )
    
    # è°ƒè¯•æ—¥å¿—ï¼šç¡®è®¤æ˜¯å¦åˆ›å»ºæ–°ä¼šè¯ï¼ˆcreated=True è¡¨ç¤ºæ–°ä¼šè¯ï¼‰
    import logging
    logger = logging.getLogger(__name__)
    
    if created:
        print(f"âœ¨ [æ•°æ®åº“æ“ä½œ] åˆ›å»ºæ–°ä¼šè¯ - ID: {session.id}, session_id: '{session.session_id}'")
        print(f"âœ¨ [æ–°ä¼šè¯è¯¦æƒ…] ç”¨æˆ·: {session.user.user}, ä¸Šä¸‹æ–‡: ç©º")
        logger.info(f"ä¼šè¯ {session_id}ï¼ˆç”¨æˆ·ï¼š{user.user}ï¼‰åˆ›å»ºæ–°ä¼šè¯")
    else:
        print(f"ğŸ“‚ [æ•°æ®åº“æ“ä½œ] åŠ è½½ç°æœ‰ä¼šè¯ - ID: {session.id}, session_id: '{session.session_id}'")
        print(f"ğŸ“‚ [ç°æœ‰ä¼šè¯è¯¦æƒ…] ç”¨æˆ·: {session.user.user}, ä¸Šä¸‹æ–‡é•¿åº¦: {len(session.context)} å­—ç¬¦")
        print(f"ğŸ“‚ [ä¼šè¯åˆ›å»ºæ—¶é—´] {session.created_at}")
        print(f"ğŸ“‚ [ä¼šè¯æ›´æ–°æ—¶é—´] {session.updated_at}")
        if session.context:
            print(f"ğŸ“‚ [å†å²ä¸Šä¸‹æ–‡é¢„è§ˆ] {session.context[:150]}{'...' if len(session.context) > 150 else ''}")
        logger.info(f"ä¼šè¯ {session_id}ï¼ˆç”¨æˆ·ï¼š{user.user}ï¼‰åŠ è½½æ—§ä¼šè¯")
    
    return session

def get_cached_reply(prompt: str, session_id: str, user: APIKey) -> str | None:
    """ç¼“å­˜é”®åŒ…å« session_id å’Œ userï¼Œé¿å…è·¨ä¼šè¯å†²çª"""
    cache_key = f"reply:{user.user}:{session_id}:{hash(prompt)}"
    print(f"ğŸ” [ç¼“å­˜æŸ¥è¯¢] ç¼“å­˜é”®: {cache_key}")
    
    cached_result = cache.get(cache_key)
    if cached_result:
        print(f"âœ… [ç¼“å­˜å‘½ä¸­] æ‰¾åˆ°ç¼“å­˜å›å¤ï¼Œé•¿åº¦: {len(cached_result)} å­—ç¬¦")
        print(f"ğŸ’¾ [ç¼“å­˜å†…å®¹] {cached_result[:80]}{'...' if len(cached_result) > 80 else ''}")
    else:
        print(f"âŒ [ç¼“å­˜æœªå‘½ä¸­] ç¼“å­˜ä¸­æ²¡æœ‰æ‰¾åˆ°å¯¹åº”å›å¤")
    
    return cached_result

def set_cached_reply(prompt: str, reply: str, session_id: str, user: APIKey, timeout=3600):
    cache_key = f"reply:{user.user}:{session_id}:{hash(prompt)}"
    print(f"ğŸ’¾ [ç¼“å­˜ä¿å­˜] ä¿å­˜å›å¤åˆ°ç¼“å­˜")
    print(f"ğŸ’¾ [ç¼“å­˜é”®] {cache_key}")
    print(f"ğŸ’¾ [ç¼“å­˜å†…å®¹] é•¿åº¦: {len(reply)} å­—ç¬¦, è¿‡æœŸæ—¶é—´: {timeout}ç§’")
    print(f"ğŸ’¾ [å›å¤é¢„è§ˆ] {reply[:80]}{'...' if len(reply) > 80 else ''}")
    
    cache.set(cache_key, reply, timeout)
    print(f"âœ… [ç¼“å­˜å®Œæˆ] å›å¤å·²æˆåŠŸä¿å­˜åˆ°ç¼“å­˜")


def generate_cache_key(original_key: str) -> str:
    """
    ç”Ÿæˆå®‰å…¨çš„ç¼“å­˜é”®ã€‚
    å¯¹åŸå§‹å­—ç¬¦ä¸²è¿›è¡Œå“ˆå¸Œå¤„ç†ï¼Œç¡®ä¿é”®é•¿åº¦å›ºå®šä¸”ä»…åŒ…å«å®‰å…¨å­—ç¬¦ã€‚
    """
    # ä½¿ç”¨SHA256å“ˆå¸Œå‡½æ•°ç”Ÿæˆå›ºå®šé•¿åº¦çš„é”®ï¼ˆ64ä½åå…­è¿›åˆ¶å­—ç¬¦ä¸²ï¼‰
    hash_obj = hashlib.sha256(original_key.encode('utf-8'))
    return hash_obj.hexdigest()
